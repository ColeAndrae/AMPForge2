{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3cac6ee-cc16-4367-91ab-a047dd9ce2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available and set as device.\n"
     ]
    }
   ],
   "source": [
    "# Import Statments:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Adding Device Management:\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available and set as device.\")\n",
    "else:\n",
    "    print(\"MPS is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9f734ce-1ecb-421b-925d-831c501377d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 8, 14, 16,  6,  3, 17,  2, 12],\n",
       "         [ 6, 20, 17, 15, 13,  8, 13, 15],\n",
       "         [10,  2,  5, 10, 13, 10,  1,  1],\n",
       "         [ 9,  1, 18,  1,  8,  2, 10, 15]], device='mps:0'),\n",
       " tensor([[14, 16,  6,  3, 17,  2, 12,  2],\n",
       "         [20, 17, 15, 13,  8, 13, 15, 13],\n",
       "         [ 2,  5, 10, 13, 10,  1,  1, 10],\n",
       "         [ 1, 18,  1,  8,  2, 10, 15,  4]], device='mps:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Curated AMP.txt file from https://aps.unmc.edu/:\n",
    "with open('AMP.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenizing Amino Acids:\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "itos = {ch:i for ch,i in enumerate(chars)}\n",
    "stoi = {i:ch for ch,i in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[n] for n in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "\n",
    "# Creating Training/Validation Split:\n",
    "n = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Creating Training Batches:\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    yb = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab604935-821f-4b42-bd7d-b409e252ae99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1357,  0.8714,  1.3087,  ...,  1.6417, -2.6251,  1.4841],\n",
       "        [-1.2811, -0.1951,  1.6697,  ..., -0.4239, -0.1126,  0.8062],\n",
       "        [ 0.7110, -0.4583,  0.6583,  ...,  0.8311,  1.2196,  0.8259],\n",
       "        ...,\n",
       "        [ 0.3012, -2.1751,  0.9615,  ..., -0.3972, -0.8386, -1.4481],\n",
       "        [ 1.4901,  0.8245, -0.1473,  ...,  1.0899,  0.1246, -0.3761],\n",
       "        [-0.3681, -0.2876, -0.1821,  ..., -0.6508,  1.6225, -0.4149]],\n",
       "       device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Hyperparameters:\n",
    "n_embd = 256\n",
    "head_size = 16\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "dropout = 0.2\n",
    "\n",
    "# Single Head of Attention:\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # K,Q,V Matrices:\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # Buffer Matrix and Dropout Layer:\n",
    "        self.register_buffer('tril', torch.tril(torch.ones([block_size, block_size])))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        \n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # Determining Affinities with Weighted Sum:\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Adjusting Embedding With Value Matrix:\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "# Parralelization of Attention Heads:\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, n_head):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
    "\n",
    "        # Projection and Dropout Layers:\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# Multi-Layer Perceptron:\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear Layers:\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Self-Attention/MLP Block:\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "\n",
    "        # Self-Attention/MLP:\n",
    "        self.sa = MultiHeadedAttention(head_size, n_head)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        # Layer Normalization:\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    # Residual Blocks:\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# AMP Transformer Model:\n",
    "class AMPTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token and Positional Embedding Tables:\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Block Layers:\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "\n",
    "        # Layer Normalization and Unembedding:\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        # Embedding:\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Creating Logits after Forward Pass:\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Determining Loss via Cross Entropy\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx):\n",
    "\n",
    "        # Generate New Data Until End Token:\n",
    "        while True:\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=1)\n",
    "            if idx_new == 0:\n",
    "                break\n",
    "\n",
    "        return idx\n",
    "        \n",
    "# Initializing Model\n",
    "m = AMPTransformer()\n",
    "m = m.to(device)\n",
    "m = torch.compile(m)\n",
    "\n",
    "# Creating Optimizer:\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=3e-4)\n",
    "\n",
    "list(m.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a005d1d1-c3b6-4dc2-9640-d9b9f15cf776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: 3.2003\n",
      "step: 100 loss: 2.8432\n",
      "step: 200 loss: 2.8349\n",
      "step: 300 loss: 2.7396\n",
      "step: 400 loss: 2.7642\n",
      "step: 500 loss: 2.7089\n",
      "step: 600 loss: 2.7032\n",
      "step: 700 loss: 2.6772\n",
      "step: 800 loss: 2.6437\n",
      "step: 900 loss: 2.6416\n",
      "step: 1000 loss: 2.6051\n",
      "step: 1100 loss: 2.5307\n",
      "step: 1200 loss: 2.4992\n",
      "step: 1300 loss: 2.5500\n",
      "step: 1400 loss: 2.5299\n",
      "step: 1500 loss: 2.4315\n",
      "step: 1600 loss: 2.2681\n",
      "step: 1700 loss: 2.4602\n",
      "step: 1800 loss: 2.2237\n",
      "step: 1900 loss: 2.2500\n",
      "step: 2000 loss: 2.2130\n",
      "step: 2100 loss: 2.2151\n",
      "step: 2200 loss: 2.0892\n",
      "step: 2300 loss: 2.1088\n",
      "step: 2400 loss: 2.2408\n",
      "step: 2500 loss: 2.0344\n",
      "step: 2600 loss: 2.1426\n",
      "step: 2700 loss: 1.8475\n",
      "step: 2800 loss: 1.8415\n",
      "step: 2900 loss: 1.8736\n",
      "step: 3000 loss: 1.8024\n",
      "step: 3100 loss: 1.8581\n",
      "step: 3200 loss: 1.7964\n",
      "step: 3300 loss: 1.7662\n",
      "step: 3400 loss: 1.7608\n",
      "step: 3500 loss: 1.7706\n",
      "step: 3600 loss: 1.6799\n",
      "step: 3700 loss: 1.6009\n",
      "step: 3800 loss: 1.5606\n",
      "step: 3900 loss: 1.6539\n",
      "step: 4000 loss: 1.5692\n",
      "step: 4100 loss: 1.4567\n",
      "step: 4200 loss: 1.4488\n",
      "step: 4300 loss: 1.4572\n",
      "step: 4400 loss: 1.3007\n",
      "step: 4500 loss: 1.3099\n",
      "step: 4600 loss: 1.3611\n",
      "step: 4700 loss: 1.4626\n",
      "step: 4800 loss: 1.3338\n",
      "step: 4900 loss: 1.1464\n",
      "step: 5000 loss: 1.2616\n",
      "step: 5100 loss: 1.1890\n",
      "step: 5200 loss: 1.0925\n",
      "step: 5300 loss: 1.0630\n",
      "step: 5400 loss: 1.1606\n",
      "step: 5500 loss: 1.0679\n",
      "step: 5600 loss: 1.0951\n",
      "step: 5700 loss: 1.1321\n",
      "step: 5800 loss: 1.0728\n",
      "step: 5900 loss: 0.9262\n",
      "step: 6000 loss: 1.0401\n",
      "step: 6100 loss: 0.9469\n",
      "step: 6200 loss: 0.9951\n",
      "step: 6300 loss: 0.9740\n",
      "step: 6400 loss: 0.9424\n",
      "step: 6500 loss: 0.8704\n",
      "step: 6600 loss: 0.9071\n",
      "step: 6700 loss: 0.8538\n",
      "step: 6800 loss: 0.9056\n",
      "step: 6900 loss: 0.8813\n",
      "step: 7000 loss: 0.8275\n",
      "step: 7100 loss: 0.8565\n",
      "step: 7200 loss: 0.8625\n",
      "step: 7300 loss: 0.8028\n",
      "step: 7400 loss: 0.7803\n",
      "step: 7500 loss: 0.7672\n",
      "step: 7600 loss: 0.7641\n",
      "step: 7700 loss: 0.7127\n",
      "step: 7800 loss: 0.7799\n",
      "step: 7900 loss: 0.7781\n",
      "step: 8000 loss: 0.7123\n",
      "step: 8100 loss: 0.7369\n",
      "step: 8200 loss: 0.7534\n",
      "step: 8300 loss: 0.6912\n",
      "step: 8400 loss: 0.7487\n",
      "step: 8500 loss: 0.6638\n",
      "step: 8600 loss: 0.6784\n",
      "step: 8700 loss: 0.6935\n",
      "step: 8800 loss: 0.6469\n",
      "step: 8900 loss: 0.6777\n",
      "step: 9000 loss: 0.6495\n",
      "step: 9100 loss: 0.6056\n",
      "step: 9200 loss: 0.6671\n",
      "step: 9300 loss: 0.6604\n",
      "step: 9400 loss: 0.6489\n",
      "step: 9500 loss: 0.5880\n",
      "step: 9600 loss: 0.6214\n",
      "step: 9700 loss: 0.6114\n",
      "step: 9800 loss: 0.5614\n",
      "step: 9900 loss: 0.6131\n",
      "total loss: 0.5853\n"
     ]
    }
   ],
   "source": [
    "# Creating Training Loop:\n",
    "steps = 10000\n",
    "\n",
    "for step in range(steps):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print(f'step: {step} loss: {loss:.4f}')\n",
    "\n",
    "print(f'total loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41e84f0c-2115-4667-93fe-95c05ac638f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GCVKVNGNVGGSLNGKAKTAISAGVAAGTVEWGFVSKTYYKGPNFEIPKGKIVCYTVSWGYAGNNTYNIASVWDLLCLTSPGWGTIIVGATAVGNMTFASGGIKH\n",
      "\n",
      "\n",
      "AIKYDSKKLDPSQVKQKKKVQKK\n",
      "\n",
      "\n",
      "RRLHQGVRNGKRPQHMYGKFYDAKMHLPYPCRQKVVNWLLLTIQTVVPLKQ\n",
      "\n",
      "\n",
      "NLKTYPKPTPQKFPTPYEHPIILPNGPNFPSQELGGAPKCALNCVTESDPPLIAGCKACCLDPHTCEPTHHICKLLCKDLS\n",
      "\n",
      "\n",
      "SAVILDTLKAAGKGALQGLLSTASCKLKNMASGC\n",
      "\n",
      "\n",
      "YGSEDVCFKPKCPDGQLICGKPFKCECFDSHSCKCPLNKVCLDPI\n",
      "\n",
      "\n",
      "CTCPDLSLKSKFVNDAKCKTITQELCAKSEKNGSKKNCWDKRRSELLDRPPR\n",
      "\n",
      "\n",
      "TSLLEPDDKKLIQMGPTVSPKILNEKSKIAYGFTNISNIKEWQSTSCNDLKWHSPWNPTACELLNTYSCNCEKFLHDDICAKKVDGRDVRDAVIVVVLDSGIGGGVSPDFGNNLFGHNTSGSEYSSSSLSYSVTYKSSGSLSS\n",
      "\n",
      "\n",
      "FLGKMKVNFGPAIMAIAKHFAKKHL\n",
      "\n",
      "\n",
      "GFFTAYCDVVSKKCAAAHMNKRRCKLTGCKPKDYS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating Novel AMP Sequences:\n",
    "sequences = []\n",
    "\n",
    "def generate(n, min_length):\n",
    "    for _ in range(n):\n",
    "        new_acid_size = 0\n",
    "        while new_acid_size < min_length:\n",
    "            idx = torch.zeros([1, 1], dtype=torch.long).to(device)\n",
    "            new_acid = decode(m.generate(idx)[0].tolist())\n",
    "            new_acid_size = len(new_acid)\n",
    "        print(new_acid)\n",
    "        sequences.append(new_acid)\n",
    "\n",
    "generate(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b1921-d1e0-436c-8d51-c92dcb389b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
