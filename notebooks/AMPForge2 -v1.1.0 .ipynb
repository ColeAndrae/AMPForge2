{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa90fbae-6d49-43b9-8a6e-d14c9f639b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available and set as device.\n"
     ]
    }
   ],
   "source": [
    "# Import Statments:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Adding Device Management:\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available and set as device.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available and set as device.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14e8584f-829a-4379-8439-9acc7558d408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5284250, 1447687, 1447687)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptides = []\n",
    "amps = []\n",
    "non_amps = []\n",
    "\n",
    "# Reading Generalized Peptide File from PeptideAtlas:\n",
    "with open('APD_Hs_all.fasta', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if not line.startswith('>'):\n",
    "            for c in line:\n",
    "                    peptides.append(c)\n",
    "\n",
    "# Tokenizing Amino Acids:\n",
    "amino_acids = sorted(list(set(peptides)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Reading AMP File from dbAMP:\n",
    "with open('dbAMP3.fasta', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if not line.startswith('>'):\n",
    "            for c in line:\n",
    "                if c in amino_acids:\n",
    "                    amps.append(c)\n",
    "\n",
    "# Reading non-AMP File from UniProt:\n",
    "with open('uniprotkb_non_amp.fasta', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if not line.startswith('>'):\n",
    "            for c in line:\n",
    "                if c in amino_acids:\n",
    "                    non_amps.append(c)\n",
    "\n",
    "non_amps = non_amps[:len(amps)]\n",
    "\n",
    "len(peptides), len(amps), len(non_amps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6946449-c74b-4db1-a395-73a56cd115e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5284250, 528425, 1447687, 144769, 1447687, 144769)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itoaa = {aa:i for aa,i in enumerate(amino_acids)}\n",
    "aatoi = {i:aa for aa,i in enumerate(amino_acids)}\n",
    "\n",
    "encode = lambda l: [aatoi[aa] for aa in l]\n",
    "decode = lambda l: ''.join([itoaa[i] for i in l])\n",
    "\n",
    "# Creating Training/Validation Split:\n",
    "n1 = int(0.9 * len(peptides))\n",
    "n2 = int(0.9 * len(amps))\n",
    "\n",
    "# Generalized Peptide Data:\n",
    "\n",
    "peptide_data = torch.tensor(encode(peptides), dtype=torch.long).to(device)\n",
    "\n",
    "peptide_train_data = peptide_data[:n1]\n",
    "peptide_val_data = peptide_data[n1:]\n",
    "\n",
    "# AMP Data:\n",
    "amp_data = torch.tensor(encode(amps), dtype=torch.long).to(device)\n",
    "\n",
    "amp_train_data = amp_data[:n2]\n",
    "amp_val_data = amp_data[n2:]\n",
    "\n",
    "# non-AMP Data:\n",
    "non_amp_data = torch.tensor(encode(non_amps), dtype=torch.long).to(device)\n",
    "\n",
    "non_amp_train_data = non_amp_data[:n2]\n",
    "non_amp_val_data = non_amp_data[n2:]\n",
    "\n",
    "len(peptide_data), len(peptide_val_data), len(amp_data), len(amp_val_data), len(non_amp_data), len(non_amp_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c1d622ca-44c3-4008-b3ed-15fd0b2a2e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2107,  2.0431,  0.3023,  ..., -0.2195,  0.4472,  0.3479],\n",
       "        [ 0.1121,  0.1404, -0.3309,  ..., -0.9215,  0.4634,  0.7284],\n",
       "        [-0.4974, -1.6729,  0.0614,  ...,  0.8258,  0.6698,  0.5455],\n",
       "        ...,\n",
       "        [-2.4245,  1.1061, -0.0743,  ...,  0.2002, -0.2998, -0.9666],\n",
       "        [-1.0869, -0.5832,  0.5542,  ...,  1.1487, -1.4038,  0.6769],\n",
       "        [-1.1748,  0.6589, -0.3503,  ...,  1.2515,  0.3509,  0.3113]],\n",
       "       device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Hyperparameters:\n",
    "n_embd = 128\n",
    "head_size = 16\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "dropout = 0.2\n",
    "\n",
    "# Single Head of Attention:\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # K,Q,V Matrices:\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # Buffer Matrix and Dropout Layer:\n",
    "        self.register_buffer('tril', torch.tril(torch.ones([block_size, block_size])))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        \n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # Determining Affinities with Weighted Sum:\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Adjusting Embedding With Value Matrix:\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "# Parralelization of Attention Heads:\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, n_head):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
    "\n",
    "        # Projection and Dropout Layers:\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# Multi-Layer Perceptron:\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear Layers:\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Self-Attention/MLP Block:\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "\n",
    "        # Self-Attention/MLP:\n",
    "        self.sa = MultiHeadedAttention(head_size, n_head)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        # Layer Normalization:\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    # Residual Blocks:\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# AMP Transformer Model:\n",
    "class AMPTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token and Positional Embedding Tables:\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Block Layers:\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "\n",
    "        # Layer Normalization and Unembedding:\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # Projection Head for Contrastive Learning:\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_embd, n_embd // 2),\n",
    "        )\n",
    "\n",
    "    def get_embeddings(self, idx):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        # Embedding:\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Creating Logits after Forward Pass:\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        sequence_emb = torch.mean(x, dim=1)\n",
    "\n",
    "        contrastive_emb = self.projection_head(sequence_emb)\n",
    "        contrastive_emb = F.normalize(contrastive_emb, p=2, dim=1)\n",
    "            \n",
    "        return contrastive_emb\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        # Embedding:\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Creating Logits after Forward Pass:\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Determining Loss via Cross Entropy:\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx):\n",
    "\n",
    "        # Generate New Data Until End Token:\n",
    "        while True:\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=1)\n",
    "            if idx_new == 0:\n",
    "                break\n",
    "\n",
    "        return idx\n",
    "\n",
    "def contrastive_loss(embeddings_amp, embeddings_non_amp, temperature=0.07):\n",
    "        batch_size = embeddings_non_amp.size(0)\n",
    "        \n",
    "        # Concatenating AMP and non-AMP Embeddings:\n",
    "        all_embeddings = torch.cat([embeddings_amp, embeddings_non_amp], dim=0)\n",
    "\n",
    "        # Creating Contrastive Labels:\n",
    "        labels = torch.cat([\n",
    "            torch.zeros(batch_size, dtype=torch.long, device=embeddings_amp.device),\n",
    "            torch.ones(batch_size, dtype=torch.long, device=embeddings_amp.device),\n",
    "        ])\n",
    "\n",
    "        # Similarity Matrix:\n",
    "        similarity_matrix = all_embeddings @ all_embeddings.T / temperature\n",
    "\n",
    "        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=device)\n",
    "        similarity_matrix = similarity_matrix.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        labels_expanded = labels.unsqueeze(0)\n",
    "        positive_mask = (labels_expanded == labels_expanded.T) & ~mask\n",
    "        negative_mask = (labels_expanded != labels_expanded.T) & ~mask\n",
    "    \n",
    "        # InfoNCE loss\n",
    "        exp_sim = torch.exp(similarity_matrix)\n",
    "    \n",
    "        # Positive Pairs:\n",
    "        positive_sim = exp_sim * positive_mask.float()\n",
    "        positive_sum = torch.sum(positive_sim, dim=1)\n",
    "    \n",
    "        # All pairs:\n",
    "        all_sum = torch.sum(exp_sim, dim=1)\n",
    "    \n",
    "        # Contrastive loss:\n",
    "        loss = -torch.log(positive_sum / (all_sum + 1e-8))\n",
    "        loss = torch.mean(loss[positive_sum > 0])  # Only compute loss where positive pairs exist\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "# Initializing Model:\n",
    "m = AMPTransformer()\n",
    "m = m.to(device)\n",
    "m = torch.compile(m)\n",
    "\n",
    "# Creating Optimizer:\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=3e-4)\n",
    "\n",
    "list(m.parameters())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7079aaa2-a520-4228-90f7-2e5b6fcc9a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7978944778442383\n",
      "2.5765628814697266\n",
      "2.6531050205230713\n",
      "2.5218868255615234\n",
      "2.48783016204834\n",
      "2.210139751434326\n",
      "2.3059821128845215\n",
      "2.2393100261688232\n",
      "2.3452038764953613\n",
      "2.3197412490844727\n"
     ]
    }
   ],
   "source": [
    "# Batching Training and Validation Data:\n",
    "def get_peptide_batch(split):\n",
    "    data = peptide_train_data if split == 'train' else peptide_val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    yb = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return xb, yb\n",
    "\n",
    "# Batching AMP and non-AMP Data:\n",
    "def get_contrast_batch(split):\n",
    "    # AMP Batch:\n",
    "    data = amp_train_data if split == 'train' else amp_val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size // 2,))\n",
    "    amp_sequence = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    amp_sequence = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    # non-AMP Batch:\n",
    "    data = amp_train_data if split == 'train' else amp_val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size // 2,))\n",
    "    non_amp_sequence = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    non_amp_sequence = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return amp_sequence, non_amp_sequence\n",
    "\n",
    "# Pre-Training with Generalized Peptides:\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_amp_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 1000 == 0:\n",
    "        print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4da0ea3-d583-4249-a31a-ae8631170f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.792475938796997\n",
      "2.8082025051116943\n",
      "2.716932535171509\n",
      "2.922257423400879\n",
      "2.9218149185180664\n",
      "2.716519355773926\n",
      "2.7867369651794434\n",
      "2.6095669269561768\n",
      "2.7807822227478027\n",
      "2.5426905155181885\n"
     ]
    }
   ],
   "source": [
    "# Contrastive Learning with AMP and non-AMP's\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_amp_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    contrastive_loss_val = 0\n",
    "\n",
    "    if steps % 5 == 0:\n",
    "\n",
    "        # Creating AMP and non-AMP Batches:\n",
    "        amp_sequence, non_amp_sequence = get_contrast_batch('train')\n",
    "\n",
    "        # Creating Embeddings:\n",
    "        amp_embeddings = m.get_embeddings(amp_sequence)\n",
    "        non_amp_embeddings = m.get_embeddings(non_amp_sequence)\n",
    "\n",
    "        # Finding Distinctions in Embeddings:\n",
    "        contrastive_loss_val = contrastive_loss(amp_embeddings, non_amp_embeddings, 0.07)\n",
    "\n",
    "    # Adding Contrastive Loss:\n",
    "    loss = loss + contrastive_loss_val\n",
    "        \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 1000 == 0:\n",
    "        print(loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
